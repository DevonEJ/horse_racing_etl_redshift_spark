# ETL Pipeline for Horse Racing Data - README WIP

## Project Overview

### Project Purpose

 - Explain what end use cases you'd like to prepare the data for (e.g., analytics table, app back-end, source-of-truth database, etc.)

### Input Data Dictionaries

-- Something on each file, including file size and row count

The 'Horses for Courses' datasets used in this project were downloaded originally from [data.world](https://data.world/sya/horses-for-courses)

Also Tipster Bets - https://data.world/data-society/horse-racing-tipster-bets


## ETL Pipeline

TBC
 - Inc. data quality checks
 - Inc. the points about future expansion
 
### Output Data Model
 Why did you choose the model you chose?
 
 Using two databases - one for storing the staging tables, and one for the cleaned analytics tables?
 
### Pipeline Steps 

Step 2: Explore and Assess the Data
Explore the data to identify data quality issues, like missing values, duplicate data, etc.
Document steps necessary to clean the data
Step 3: Define the Data Model
Map out the conceptual data model and explain why you chose that model
List the steps necessary to pipeline the data into the chosen data model
Step 4: Run ETL to Model the Data
Create the data pipelines and the data model
Include a data dictionary
Run data quality checks to ensure the pipeline ran as expected
Integrity constraints on the relational database (e.g., unique key, data type, etc.)
Unit tests for the scripts to ensure they are doing the right thing
Source/count checks to ensure completeness

## Example Analysis

